---
layout: default
title: Code
---

I lead my group with an <a href="https://mboehme.github.io/manifesto">Open Science and Reproducibility Policy</a>, which includes publishing the experimental infrastructure, tools, data, and the scripts to produce tables and figures. Most of the published repositories continue to be actively maintained by the community and receive external contributions. Apart from this group-level policy, I am also actively involved in community efforts to promote preregistration and artifact evaluation as a way to improve the soundness and reprodicibility of our empirical evaluations as well as the soundness of our peer review process.<br/>
<br/>
For instance, I am leading a grassroots initative to introduce a <a href="https://fuzzbench.com/blog/2021/04/22/special-issue/">novel preregistration-based publication process</a> for fuzzing research that consists of two main stages: In the first stage, the program committee (PC) evaluates all submissions based on: (i) the significance and novelty of the hypotheses or techniques and (ii) the soundness and reproducibility of the methodology specified to validate the claims or hypotheses---but explicitly not based on the strength of the (preliminary) results. These draft registered reports are presented and improved at the <a href="https://fuzzingworkshop.github.io/">FUZZINGâ€™22</a> workshop. After the workshop, the final versions of the registered reports are re-checked and approved by the PC.
In the second stage, the PC and the Artifact Evaluation Committee (AEC) check whether the experimental methodology as laid out by the authors was correctly followed. I am excited that the outcome of this stage will be published in the <a href="https://dl.acm.org/journal/tosem">ACM Transactions on Software Engineering and Methodology (TOSEM)</a> via the Preregistration track (which I have been invited to establish assisting Cristian Cadar, and I am not heading).


{% include code.html %}
